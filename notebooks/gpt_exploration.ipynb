{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Skm-M8L0syfJ"
      },
      "outputs": [],
      "source": [
        "! pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyF53ke1s7eF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VP0UD4LltABn"
      },
      "outputs": [],
      "source": [
        "encoded = tokenizer.encode(\"Do not meddle in the affairs of wizards\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3XsqkMQtO6N"
      },
      "outputs": [],
      "source": [
        "print(\"Encoded text:\", tokenizer.convert_ids_to_tokens(encoded))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NdmylpO0tTWt"
      },
      "outputs": [],
      "source": [
        "print(encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25G-Qm1wta7v"
      },
      "outputs": [],
      "source": [
        "encoded_input = tokenizer(\"Do not meddle in the affairs of wizards, for they are subtle and quick to anger.\")\n",
        "print(encoded_input.keys())\n",
        "print(encoded_input['input_ids'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ps2vCs-3yQmm"
      },
      "outputs": [],
      "source": [
        "! pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqqPuLfDz4Xo"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "#Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgvUUF_q0Mmi"
      },
      "outputs": [],
      "source": [
        "words = [\"quick\", \"fast\", \"red\", \"blue\", \"ferari\"]\n",
        "single_word_embeddings = model.encode(words)\n",
        "\n",
        "for word, embed in zip(words, single_word_embeddings):\n",
        "  print(\"word: \", word)\n",
        "  print(\"embed: \", embed[0:10])\n",
        "  print(\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96EPNlK65mWl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "cos_sim = cosine_similarity(single_word_embeddings)\n",
        "print(cos_sim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsVV5lSf8hoo"
      },
      "outputs": [],
      "source": [
        "from scipy.spatial.distance import cosine\n",
        "for sentence in sentences:\n",
        "    print(\"Sentence:\", sentence)\n",
        "print(\"\")\n",
        "print(\"Cosine similarity between the first two sentences:\", cosine(embeddings[0], embeddings[1]))\n",
        "print(\"Cosine similarity between the second and third sentences:\", cosine(embeddings[1], embeddings[2]))\n",
        "print(\"Cosine similarity between the first and third sentences:\", cosine(embeddings[0], embeddings[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmMWI3drpWlz"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the pre-trained tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Input sentence\n",
        "sentence = \"Time after time, time flies like an arrow, but fruit flies like a banana.\"\n",
        "\n",
        "# Words of interest\n",
        "words_of_interest = [\"time\", \"flies\", \"like\"]\n",
        "\n",
        "# Tokenize the input sentence\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "\n",
        "# Initialize a dictionary to store the embeddings for each word\n",
        "word_embeddings = {word: [] for word in words_of_interest}\n",
        "\n",
        "# Disable gradient calculation for inference\n",
        "with torch.no_grad():\n",
        "    # Tokenize the input text and convert to a tensor\n",
        "    encoded_input = tokenizer(sentence, return_tensors='pt')\n",
        "\n",
        "    # Get the model's output (hidden states)\n",
        "    outputs = model(**encoded_input)\n",
        "    last_hidden_states = outputs.last_hidden_state\n",
        "\n",
        "    # Extract the embeddings for the words of interest\n",
        "    for i, token in enumerate(tokens):\n",
        "        if token.lower() in words_of_interest:\n",
        "            word_embeddings[token.lower()].append(last_hidden_states[0, i])\n",
        "\n",
        "# Compute cosine similarities between the word embeddings\n",
        "for word, embeddings in word_embeddings.items():\n",
        "    for i in range(len(embeddings)):\n",
        "        for j in range(i + 1, len(embeddings)):\n",
        "            similarity = cosine_similarity(embeddings[i].reshape(1, -1), embeddings[j].reshape(1, -1))\n",
        "            print(f\"Cosine similarity between '{word}' in position {i + 1} and position {j + 1}: {similarity[0][0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTK9rsg-QIXc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from scipy.spatial.distance import cosine\n",
        "\n",
        "# Load the tokenizer and the model\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = AutoModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Define the sentences\n",
        "sentence1 = \"Time flies like an arrow\"\n",
        "sentence2 = \"Fruit flies like a banana\"\n",
        "\n",
        "# Tokenize and encode the sentences\n",
        "encoded_sentence1 = tokenizer(sentence1, return_tensors='pt')\n",
        "encoded_sentence2 = tokenizer(sentence2, return_tensors='pt')\n",
        "\n",
        "# Identify the 'like' token index\n",
        "word = \"like\"\n",
        "word_index1 = tokenizer.encode(sentence1, add_special_tokens=True).index(tokenizer.encode(word)[1]) - 1\n",
        "word_index2 = tokenizer.encode(sentence2, add_special_tokens=True).index(tokenizer.encode(word)[1]) - 1\n",
        "\n",
        "# Get the embeddings for each encoded sentence\n",
        "with torch.no_grad(): # Disable gradient tracking\n",
        "    output_sentence1 = model(**encoded_sentence1)\n",
        "    output_sentence2 = model(**encoded_sentence2)\n",
        "\n",
        "# Extract the embeddings for the word 'like' from both sentences using their indices\n",
        "embedding_sentence1 = output_sentence1.last_hidden_state[0, word_index1]\n",
        "embedding_sentence2 = output_sentence2.last_hidden_state[0, word_index2]\n",
        "\n",
        "# Calculate the cosine similarity\n",
        "similarity = 1 - cosine(embedding_sentence1.detach().numpy(), embedding_sentence2.detach().numpy())\n",
        "\n",
        "# Print the results\n",
        "print(\"Cosine Similarity:\", similarity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rm_6NAwUr6bI"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "\n",
        "# Initialize the tokenizer and model from the BERT family\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Sentences to analyze\n",
        "sentence1 = \"Time flies like an arrow\"\n",
        "sentence2 = \"Fruit flies like a banana\"\n",
        "\n",
        "# Tokenize and encode sentences for BERT input\n",
        "tokens1 = tokenizer.encode_plus(sentence1, return_tensors='pt')\n",
        "tokens2 = tokenizer.encode_plus(sentence2, return_tensors='pt')\n",
        "\n",
        "# Get the embeddings from the BERT model\n",
        "outputs1 = model(**tokens1)\n",
        "outputs2 = model(**tokens2)\n",
        "\n",
        "# Retrieve the embeddings for the word \"like\" for each sentence\n",
        "# Assuming 'like' is not the first word and does not get split into subwords.\n",
        "like_index1 = tokens1['input_ids'][0].tolist().index(tokenizer.encode('like', add_special_tokens=False)[0])\n",
        "like_index2 = tokens2['input_ids'][0].tolist().index(tokenizer.encode('like', add_special_tokens=False)[0])\n",
        "\n",
        "like_embedding1 = outputs1.last_hidden_state[0, like_index1]\n",
        "like_embedding2 = outputs2.last_hidden_state[0, like_index2]\n",
        "\n",
        "# Compare the embeddings, e.g., by using cosine similarity\n",
        "cosine_similarity = torch.nn.CosineSimilarity(dim=0)\n",
        "similarity = cosine_similarity(like_embedding1, like_embedding2).item()\n",
        "\n",
        "print(f\"Cosine similarity between 'like' in both contexts: {similarity}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
